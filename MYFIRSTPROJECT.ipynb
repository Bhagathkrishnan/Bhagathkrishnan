** AMRITA JOBSEEKR**
#LIBRARY TO IMPORT
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import shutil
import sys
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
#BERT MODEL#
from transformers import BertModel , BertTokenizer
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score

#UPLOADE FILE#

train_df=pd.read_csv('/content/finallast.csv')

#CONCAT#

train_df['context'] = train_df['Title'] + ". " + train_df['Abstract']

train_df=train_df[['context','Enzyme Engineering', 'Biosurfactants in Industrial and Environmental Applications', 'Molecular Biology', 'Parasitology', 'Cancer Biology', 'Molecular Cell Biology', 'Virology', ' Physiology ', 'Environmental Medicine', 'Metabolomic', 'Reproductive Medicine', 'Biomaterials', 'Tissue Engineering', 'Agricultural Science', 'Environmental Sustainability', 'Medical Research', 'Cell Biology', 'Microbiology', 'Antibiotic Resistance Research', 'Plant Biochemistry', 'Proteomics', 'Malaria Research', 'Immunology', 'Plant Genetics', 'Plant Pathology', 'Environmental Science', 'Bioprocess Engineering', 'Biotechnology', 'Genetics', 'Life Sciences', 'Clinical Research', 'Biochemistry Research', 'Agricultural', 'Developmental Biology', 'Drug Development', 'Biomedical Research', 'Nanomedicine', 'Bioprinting', 'Vaccine Development', 'Epigenetics ', 'Chromatin Dynamics Research', 'Marine Microbiology', 'Natural Product Discovery', 'Metabolic Biology', 'Obesity Research', 'Probiotics Resesrch', 'Pain Physiology', 'Neurobilogy', 'Water Quality Analysis', 'Livestock Health', 'Hepatitis C', 'Liver Disease', 'Cancer Research', 'Sleep Physiology', 'Behavioral Neuroscience', 'Clinical Trials', 'Bioprocessing', 'Industrial Biotechnology', 'Neuroscience', 'Oncology', 'Public Health', 'Stem Cell Biology', 'Embryonic Stem Cells', ' Data Management', 'Preclinical Trials', 'Snake Venom', 'Botany', 'Biochemistry', 'Virology.1', 'Protien Analysis', 'Pharmaceutical Science', 'Mycology', 'Enzymology', 'Bacterial Detection', 'Biochemical Mechanisms', 'DNA Damage Response', 'Tuberculosis', 'Food Industry', 'Probiotics', ' Enzyme Profiling', 'Food Science', 'Microbial Ecology', 'Medical Genetics ', 'Medical Sciences', 'Evolution', 'Infertility Research', 'Ophthalmology', 'Clinical Laboratory Science', 'Endocrinology', 'Pharmacology', 'Pathogenic Microbiology', 'Clinical Microbiology', 'Biomedical Engineering', 'Microbial Pathogenesis', 'Antimicrobial Agents', 'Environmental Biotechnology', 'Bacterial Physiology', 'Medical Geology', 'Enzyme Production and Characterization', 'waste water treatment ', 'Biomimetics', 'Materials Science', ' Agricultural Microbiology', 'Bioremediation', 'Sanitation', 'Antimicrobial Resistance Research', 'Marine Biotechnology', 'Waste Management', 'Gut Microbiota Research', ' Sustainable Agriculture', 'Computational Biology', 'Machine Learning Prediction', 'Bioinformatics', 'Machine Learning Applications', 'Structural Bioinformatics', 'Drug Discovery', 'Analysis Software', 'Machine learning', 'Dataset Generation', 'Comparative Genomics', 'Genomics', 'Antimicrobial Drug Development']]

#MAKING TARGET LIST#

target_list=['Enzyme Engineering', 'Biosurfactants in Industrial and Environmental Applications', 'Molecular Biology', 'Parasitology', 'Cancer Biology', 'Molecular Cell Biology', 'Virology', ' Physiology ', 'Environmental Medicine', 'Metabolomic', 'Reproductive Medicine', 'Biomaterials', 'Tissue Engineering', 'Agricultural Science', 'Environmental Sustainability', 'Medical Research', 'Cell Biology', 'Microbiology', 'Antibiotic Resistance Research', 'Plant Biochemistry', 'Proteomics', 'Malaria Research', 'Immunology', 'Plant Genetics', 'Plant Pathology', 'Environmental Science', 'Bioprocess Engineering', 'Biotechnology', 'Genetics', 'Life Sciences', 'Clinical Research', 'Biochemistry Research', 'Agricultural', 'Developmental Biology', 'Drug Development', 'Biomedical Research', 'Nanomedicine', 'Bioprinting', 'Vaccine Development', 'Epigenetics ', 'Chromatin Dynamics Research', 'Marine Microbiology', 'Natural Product Discovery', 'Metabolic Biology', 'Obesity Research', 'Probiotics Resesrch', 'Pain Physiology', 'Neurobilogy', 'Water Quality Analysis', 'Livestock Health', 'Hepatitis C', 'Liver Disease', 'Cancer Research', 'Sleep Physiology', 'Behavioral Neuroscience', 'Clinical Trials', 'Bioprocessing', 'Industrial Biotechnology', 'Neuroscience', 'Oncology', 'Public Health', 'Stem Cell Biology', 'Embryonic Stem Cells', ' Data Management', 'Preclinical Trials', 'Snake Venom', 'Botany', 'Biochemistry', 'Virology.1', 'Protien Analysis', 'Pharmaceutical Science', 'Mycology', 'Enzymology', 'Bacterial Detection', 'Biochemical Mechanisms', 'DNA Damage Response', 'Tuberculosis', 'Food Industry', 'Probiotics', ' Enzyme Profiling', 'Food Science', 'Microbial Ecology', 'Medical Genetics ', 'Medical Sciences', 'Evolution', 'Infertility Research', 'Ophthalmology', 'Clinical Laboratory Science', 'Endocrinology', 'Pharmacology', 'Pathogenic Microbiology', 'Clinical Microbiology', 'Biomedical Engineering', 'Microbial Pathogenesis', 'Antimicrobial Agents', 'Environmental Biotechnology', 'Bacterial Physiology', 'Medical Geology', 'Enzyme Production and Characterization', 'waste water treatment ', 'Biomimetics', 'Materials Science', ' Agricultural Microbiology', 'Bioremediation', 'Sanitation', 'Antimicrobial Resistance Research', 'Marine Biotechnology', 'Waste Management', 'Gut Microbiota Research', ' Sustainable Agriculture', 'Computational Biology', 'Machine Learning Prediction', 'Bioinformatics', 'Machine Learning Applications', 'Structural Bioinformatics', 'Drug Discovery', 'Analysis Software', 'Machine learning', 'Dataset Generation', 'Comparative Genomics', 'Genomics', 'Antimicrobial Drug Development']

#HYPER PARAMETER#

MAX_LEN=512
TRAIN_BATCH_SIZE=8
VALID_BATCH_SIZE=8
EPOCHS=10
LEARNING_RATE=1e-5

#CREATING CUSTOM CLASS#

class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.df = df
        self.title = df['context']
        self.targets = self.df[target_list].values
        self.max_len = max_len

    def __len__(self):
        return len(self.title)

    def __getitem__(self, index):
        title = str(self.title[index])
        title = " ".join(title.split())

        inputs = self.tokenizer.encode_plus(
            title,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': inputs['input_ids'].flatten(),
            'attention_mask': inputs['attention_mask'].flatten(),
            'token_type_ids': inputs["token_type_ids"].flatten(),
            'targets': torch.FloatTensor(self.targets[index])
        }

#SPLITTING TO TRAIN AND VALIDATE#

train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=100)
train_df = train_df.reset_index()
val_df = val_df.reset_index()

train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)
valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)

#CREATE TRAIN AND VALID DATA LOADER

train_data_loader = torch.utils.data.DataLoader(
    train_dataset,
    shuffle=True,
    batch_size = TRAIN_BATCH_SIZE,
    num_workers=0
)

val_data_loader = torch.utils.data.DataLoader(
    valid_dataset,
    shuffle=False,
    batch_size = VALID_BATCH_SIZE,
    num_workers=0
)

#GPU#

device=torch.device('cuda') if torch.cuda.is_available() else torch.device ('cpu')
print(device)

#LOADING CHEAKPOINTS#

def _load_ckp(checkpoint_fpath, model, optimizer):
  checkpoint = torch.load(checkpoint_fpath)
  model.load_state_dict(checkpoint['state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer'])
  valid_loss_min= checkpoint['valid_loss_min']
  return model, optimizer, checkpoint['epoch'],valid_loss_min.item()  

def _save_ckp(state, is_best, checkpoint_path,best_model_path):
  f_path=checkpoint_path
  torch.save(state, f_path)
  if is_best:
    best_fpath = best_model_path
    shutil.copyfile(f_path,best_fpath)

#IMPORT BERT MODEL#

class BERTClass(nn.Module):
  def __init__(self):
    super(BERTClass,self).__init__()
    self.bert_model = BertModel.from_pretrained("bert-base-uncased", return_dict=True)
    self.dropout = nn.Dropout(0.3)
    self.linear = nn.Linear(768,122)

  def forward(self,input_ids,attention_mask,token_type_ids):
    output = self.bert_model(input_ids,attention_mask,token_type_ids)
    output_dropout =self.dropout(output.pooler_output)
    output = self.linear(output_dropout)
    return output

model=BERTClass()
model.to(device) 

def loss_fn(outputs, targets):
  return nn.BCEWithLogitsLoss() (outputs, targets)

optimizer = torch.optim.Adam(params=model.parameters(),lr=LEARNING_RATE) 

val_targets=[]
val_outputs=[]

#BUILD TRAIN LOOP#

def train_model(n_epochs, training_loader, validation_loader, model, 
                optimizer, checkpoint_path, best_model_path):
   
  valid_loss_min = np.Inf
   
 
  for epoch in range(1, n_epochs+1):
    train_loss = 0
    valid_loss = 0

    model.train()
    print('############# Epoch {}: Training Start   #############'.format(epoch))
    for batch_idx, data in enumerate(training_loader):
        ids = data['input_ids'].to(device, dtype = torch.long)
        mask = data['attention_mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)

        outputs = model(ids, mask, token_type_ids)

        optimizer.zero_grad()
        loss = loss_fn(outputs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))

    
    print('############# Epoch {}: Training End     #############'.format(epoch))
    
    print('############# Epoch {}: Validation Start   #############'.format(epoch))

 
    model.eval()
   
    with torch.no_grad():
      for batch_idx, data in enumerate(validation_loader, 0):
            ids = data['input_ids'].to(device, dtype = torch.long)
            mask = data['attention_mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)

            loss = loss_fn(outputs, targets)
            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))
            val_targets.extend(targets.cpu().detach().numpy().tolist())
            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())

      print('############# Epoch {}: Validation End     #############'.format(epoch))

      train_loss = train_loss/len(training_loader)
      valid_loss = valid_loss/len(validation_loader)
 
      print('Epoch: {} \tAvgerage Training Loss: {:.6f} \tAverage Validation Loss: {:.6f}'.format(
            epoch, 
            train_loss,
            valid_loss
            ))
      
      checkpoint = {
            'epoch': epoch + 1,
            'valid_loss_min': valid_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict()
      }
        

      _save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
      if valid_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))
        _save_ckp(checkpoint, True, checkpoint_path, best_model_path)
        valid_loss_min = valid_loss

    print('############# Epoch {}  Done   #############\n'.format(epoch))

  return model

#TRAIN MODEL#

trained_model = train_model(EPOCHS,train_data_loader,val_data_loader,model,optimizer,"/curr_ckpt",'/best.pt') 
